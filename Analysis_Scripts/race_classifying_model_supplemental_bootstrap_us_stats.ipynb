{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj7-WcjvLosf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import ast\n",
        "from glob import glob\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CheXpert"
      ],
      "metadata": {
        "id": "REb-UPtRLunb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Hispanic/Latino', 'Non-Hispanic Asian', 'Non-Hispanic Black',  'Non-Hispanic White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Non-Hispanic Asian',\n",
        "    'Non-Hispanic Asian': 'Non-Hispanic Asian',\n",
        "    'Black': 'Non-Hispanic Black',\n",
        "    'Non-Hispanic Black': 'Non-Hispanic Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'Non-Hispanic White',\n",
        "    'Non-Hispanic White': 'Non-Hispanic White'\n",
        "}\n",
        "\n",
        "def evaluate_race_csv(csv_file_path, label_name):\n",
        "    try:\n",
        "        predict_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Parse the stringified list into a real list\n",
        "        predict_df['Race/Ethnicity_Probability'] = predict_df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        predict_df['Race/Ethnicity'] = predict_df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        predict_df = predict_df[predict_df['Race/Ethnicity'] != 'Other']\n",
        "\n",
        "        # Encode the textual labels into integers\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true = label_encoder.fit_transform(predict_df['Race/Ethnicity'])\n",
        "\n",
        "        # Get the list of probabilities\n",
        "        y_score = predict_df['Race/Ethnicity_Probability'].tolist()\n",
        "\n",
        "        # Compute weighted AUROC\n",
        "        wAUROC = metrics.roc_auc_score(\n",
        "            y_true,\n",
        "            y_score,\n",
        "            multi_class='ovr',\n",
        "            average='weighted'\n",
        "        )\n",
        "\n",
        "        # Compute weighted AUPRC\n",
        "        wAUPRC = metrics.average_precision_score(\n",
        "            y_true,\n",
        "            y_score,\n",
        "            average='weighted'\n",
        "        )\n",
        "\n",
        "        print(f'{label_name} - wAUROC: {wAUROC:.4f}, wAUPRC: {wAUPRC:.4f}')\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "\n",
        "# Define files and labels\n",
        "csv_files_with_labels = [\n",
        "    ('/content/chexpert_race2_chexpert_test.csv', 'CheXpert'),\n",
        "    ('/content/chexpert_race2_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "# Run evaluations\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    evaluate_race_csv(file_path, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjz20ndxLuSe",
        "outputId": "1a0c0bad-f1f4-4613-bf7b-961de16928f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CheXpert - wAUROC: 0.8931, wAUPRC: 0.8367\n",
            "MIMIC - wAUROC: 0.9133, wAUPRC: 0.8824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Hispanic/Latino', 'Non-Hispanic Asian', 'Non-Hispanic Black',  'Non-Hispanic White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Non-Hispanic Asian',\n",
        "    'Non-Hispanic Asian': 'Non-Hispanic Asian',\n",
        "    'Black': 'Non-Hispanic Black',\n",
        "    'Non-Hispanic Black': 'Non-Hispanic Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'Non-Hispanic White',\n",
        "    'Non-Hispanic White': 'Non-Hispanic White'\n",
        "}\n",
        "\n",
        "def evaluate_race_csv(csv_file_path, label_name):\n",
        "    try:\n",
        "        predict_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Parse the stringified list into a real list\n",
        "        predict_df['Race/Ethnicity_Probability'] = predict_df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        predict_df['Race/Ethnicity'] = predict_df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        predict_df = predict_df[predict_df['Race/Ethnicity'] != 'Other']\n",
        "\n",
        "\n",
        "        # Encode the textual labels into integers\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true = label_encoder.fit_transform(predict_df['Race/Ethnicity'])\n",
        "        classes = label_encoder.classes_\n",
        "\n",
        "        # Get the list of probabilities and convert to numpy array\n",
        "        y_score = np.array(predict_df['Race/Ethnicity_Probability'].tolist())\n",
        "\n",
        "        # Calculate class prevalences (weights)\n",
        "        class_counts = np.bincount(y_true)\n",
        "        class_weights = class_counts / len(y_true)\n",
        "\n",
        "        # Initialize lists to store per-class metrics\n",
        "        auroc_scores = []\n",
        "        auprc_scores = []\n",
        "\n",
        "        # Compute metrics for each class using one-vs-rest approach\n",
        "        for class_idx in range(len(classes)):\n",
        "            # Create binary labels for current class\n",
        "            y_true_binary = (y_true == class_idx).astype(int)\n",
        "\n",
        "            # Get probabilities for current class\n",
        "            y_score_class = y_score[:, class_idx]\n",
        "\n",
        "            # Compute AUROC\n",
        "            if len(np.unique(y_true_binary)) >= 2:  # Need at least one positive and one negative sample\n",
        "                auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                auroc_scores.append(auroc)\n",
        "            else:\n",
        "                auroc_scores.append(np.nan)\n",
        "\n",
        "            # Compute AUPRC\n",
        "            auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "            auprc_scores.append(auprc)\n",
        "\n",
        "        # Calculate manually weighted averages (ignoring NaN values)\n",
        "        valid_auroc = ~np.isnan(auroc_scores)\n",
        "        weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "\n",
        "        weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "        # Print results\n",
        "        print(f'\\n{label_name} Results:')\n",
        "        print('Class\\t\\tPrevalence\\tAUROC\\t\\tAUPRC')\n",
        "        for i, class_name in enumerate(classes):\n",
        "            print(f'{class_name:<15}{class_weights[i]:.4f}\\t\\t{auroc_scores[i]:.4f}\\t\\t{auprc_scores[i]:.4f}')\n",
        "\n",
        "        print(f'\\n{label_name} - Manually Weighted Average - wAUROC: {weighted_auroc:.4f}, wAUPRC: {weighted_auprc:.4f}')\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "\n",
        "# Define files and labels\n",
        "csv_files_with_labels = [\n",
        "    ('/content/chexpert_race2_chexpert_test.csv', 'CheXpert'),\n",
        "    ('/content/chexpert_race2_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "# Run evaluations\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    evaluate_race_csv(file_path, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQzJVVWSPGlx",
        "outputId": "6c9d8441-5cf3-4a6b-8530-7140dc15abc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert Results:\n",
            "Class\t\tPrevalence\tAUROC\t\tAUPRC\n",
            "Hispanic/Latino0.1360\t\t0.8057\t\t0.4892\n",
            "Non-Hispanic Asian0.1432\t\t0.9290\t\t0.7646\n",
            "Non-Hispanic Black0.0573\t\t0.9491\t\t0.6827\n",
            "Non-Hispanic White0.6635\t\t0.8984\t\t0.9368\n",
            "\n",
            "CheXpert - Manually Weighted Average - wAUROC: 0.8931, wAUPRC: 0.8367\n",
            "\n",
            "MIMIC Results:\n",
            "Class\t\tPrevalence\tAUROC\t\tAUPRC\n",
            "Hispanic/Latino0.0549\t\t0.6975\t\t0.1714\n",
            "Non-Hispanic Asian0.0383\t\t0.9436\t\t0.6549\n",
            "Non-Hispanic Black0.1764\t\t0.9466\t\t0.8227\n",
            "Non-Hispanic White0.7303\t\t0.9199\t\t0.9623\n",
            "\n",
            "MIMIC - Manually Weighted Average - wAUROC: 0.9133, wAUPRC: 0.8824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "STANDARD_RACES = ['Hispanic/Latino', 'Non-Hispanic Asian', 'Non-Hispanic Black',  'Non-Hispanic White']\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Non-Hispanic Asian',\n",
        "    'Non-Hispanic Asian': 'Non-Hispanic Asian',\n",
        "    'Black': 'Non-Hispanic Black',\n",
        "    'Non-Hispanic Black': 'Non-Hispanic Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'Non-Hispanic White',\n",
        "    'Non-Hispanic White': 'Non-Hispanic White'\n",
        "}\n",
        "\n",
        "def bootstrap_race_metrics(csv_file_path, label_name, n_iterations=1000, random_state=2025):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        df['Race/Ethnicity_Probability'] = df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        df['Race/Ethnicity'] = df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        df = df[df['Race/Ethnicity'] != 'Other'].copy()\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true_all = label_encoder.fit_transform(df['Race/Ethnicity'])\n",
        "        y_score_all = np.array(df['Race/Ethnicity_Probability'].tolist())\n",
        "        classes = label_encoder.classes_\n",
        "        n_classes = len(classes)\n",
        "\n",
        "        data = pd.DataFrame({'y_true': y_true_all.tolist(), 'y_score': y_score_all.tolist()})\n",
        "        rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "        auroc_bootstrap = []\n",
        "        auprc_bootstrap = []\n",
        "\n",
        "        for _ in range(n_iterations):\n",
        "            sample = data.sample(n=len(data), replace=True, random_state=rng.integers(1e9))\n",
        "            y_true = np.array(sample['y_true'].tolist())\n",
        "            y_score = np.array(sample['y_score'].tolist())\n",
        "\n",
        "            auroc_scores = []\n",
        "            auprc_scores = []\n",
        "            class_counts = np.bincount(y_true, minlength=n_classes)\n",
        "            class_weights = class_counts / len(y_true)\n",
        "\n",
        "            for class_idx in range(n_classes):\n",
        "                y_true_binary = (y_true == class_idx).astype(int)\n",
        "                y_score_class = y_score[:, class_idx]\n",
        "\n",
        "                if len(np.unique(y_true_binary)) >= 2:\n",
        "                    auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                else:\n",
        "                    auroc = np.nan\n",
        "                auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "\n",
        "                auroc_scores.append(auroc)\n",
        "                auprc_scores.append(auprc)\n",
        "\n",
        "            # Weighted average ignoring NaN in AUROC\n",
        "            valid_auroc = ~np.isnan(auroc_scores)\n",
        "            weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "            weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "            auroc_bootstrap.append(weighted_auroc)\n",
        "            auprc_bootstrap.append(weighted_auprc)\n",
        "\n",
        "        # Compute confidence intervals\n",
        "        auroc_ci = np.percentile(auroc_bootstrap, [2.5, 97.5])\n",
        "        auprc_ci = np.percentile(auprc_bootstrap, [2.5, 97.5])\n",
        "\n",
        "        print(f\"\\n{label_name} - Bootstrapped Weighted Metrics:\")\n",
        "        print(f\"wAUROC 95% CI: [{auroc_ci[0]:.4f}, {auroc_ci[1]:.4f}]\")\n",
        "        print(f\"wAUPRC 95% CI: [{auprc_ci[0]:.4f}, {auprc_ci[1]:.4f}]\")\n",
        "\n",
        "        return auroc_ci, auprc_ci\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Run bootstrap evaluation\n",
        "csv_files_with_labels = [\n",
        "    ('/content/chexpert_race2_chexpert_test.csv', 'CheXpert'),\n",
        "    ('/content/chexpert_race2_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    bootstrap_race_metrics(file_path, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl1DX4YlH21N",
        "outputId": "2a0902ca-8e68-4a77-ec31-8c5833e8377f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.8895, 0.8965]\n",
            "wAUPRC 95% CI: [0.8323, 0.8414]\n",
            "\n",
            "MIMIC - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.9101, 0.9162]\n",
            "wAUPRC 95% CI: [0.8789, 0.8863]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "STANDARD_RACES = ['Hispanic/Latino', 'Non-Hispanic Asian', 'Non-Hispanic Black',  'Non-Hispanic White']\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Non-Hispanic Asian',\n",
        "    'Non-Hispanic Asian': 'Non-Hispanic Asian',\n",
        "    'Black': 'Non-Hispanic Black',\n",
        "    'Non-Hispanic Black': 'Non-Hispanic Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'Non-Hispanic White',\n",
        "    'Non-Hispanic White': 'Non-Hispanic White'\n",
        "}\n",
        "\n",
        "def bootstrap_race_metrics(csv_file_path, label_name, n_iterations=1000, random_state=2025):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        df['Race/Ethnicity_Probability'] = df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        df['Race/Ethnicity'] = df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        df = df[df['Race/Ethnicity'] != 'Other'].copy()\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true_all = label_encoder.fit_transform(df['Race/Ethnicity'])\n",
        "        y_score_all = np.array(df['Race/Ethnicity_Probability'].tolist())\n",
        "        classes = label_encoder.classes_\n",
        "        n_classes = len(classes)\n",
        "\n",
        "        data = pd.DataFrame({'y_true': y_true_all.tolist(), 'y_score': y_score_all.tolist()})\n",
        "        rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "        auroc_bootstrap = []\n",
        "        auprc_bootstrap = []\n",
        "        per_class_aurocs = [[] for _ in range(n_classes)]\n",
        "\n",
        "        for _ in range(n_iterations):\n",
        "            sample = data.sample(n=len(data), replace=True, random_state=rng.integers(1e9))\n",
        "            y_true = np.array(sample['y_true'].tolist())\n",
        "            y_score = np.array(sample['y_score'].tolist())\n",
        "\n",
        "            auroc_scores = []\n",
        "            auprc_scores = []\n",
        "            class_counts = np.bincount(y_true, minlength=n_classes)\n",
        "            class_weights = class_counts / len(y_true)\n",
        "\n",
        "            for class_idx in range(n_classes):\n",
        "                y_true_binary = (y_true == class_idx).astype(int)\n",
        "                y_score_class = y_score[:, class_idx]\n",
        "\n",
        "                if len(np.unique(y_true_binary)) >= 2:\n",
        "                    auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                else:\n",
        "                    auroc = np.nan\n",
        "                auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "\n",
        "                auroc_scores.append(auroc)\n",
        "                auprc_scores.append(auprc)\n",
        "\n",
        "                # Store for per-class CI\n",
        "                per_class_aurocs[class_idx].append(auroc)\n",
        "\n",
        "            valid_auroc = ~np.isnan(auroc_scores)\n",
        "            weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "            weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "            auroc_bootstrap.append(weighted_auroc)\n",
        "            auprc_bootstrap.append(weighted_auprc)\n",
        "\n",
        "        auroc_ci = np.percentile(auroc_bootstrap, [2.5, 97.5])\n",
        "        auprc_ci = np.percentile(auprc_bootstrap, [2.5, 97.5])\n",
        "\n",
        "        print(f\"\\n{label_name} - Bootstrapped Weighted Metrics:\")\n",
        "        print(f\"wAUROC 95% CI: [{auroc_ci[0]:.4f}, {auroc_ci[1]:.4f}]\")\n",
        "        print(f\"wAUPRC 95% CI: [{auprc_ci[0]:.4f}, {auprc_ci[1]:.4f}]\\n\")\n",
        "\n",
        "        # Per-class AUROC confidence intervals\n",
        "        print(f\"{label_name} - Per-Class AUROC 95% CIs:\")\n",
        "        for i, class_name in enumerate(classes):\n",
        "            class_aurocs = np.array(per_class_aurocs[i])\n",
        "            class_aurocs = class_aurocs[~np.isnan(class_aurocs)]\n",
        "            if len(class_aurocs) == 0:\n",
        "                print(f\"{class_name:<25} Insufficient data for AUROC\")\n",
        "            else:\n",
        "                ci = np.percentile(class_aurocs, [2.5, 97.5])\n",
        "                print(f\"{class_name:<25} AUROC 95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
        "\n",
        "        return auroc_ci, auprc_ci\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Run bootstrap evaluation\n",
        "csv_files_with_labels = [\n",
        "    ('/content/chexpert_race2_chexpert_test.csv', 'CheXpert'),\n",
        "    ('/content/chexpert_race2_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    bootstrap_race_metrics(file_path, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhy3bJgDQ35I",
        "outputId": "f356d34b-13f1-48e9-9c98-368909b0e041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.8895, 0.8965]\n",
            "wAUPRC 95% CI: [0.8323, 0.8414]\n",
            "\n",
            "CheXpert - Per-Class AUROC 95% CIs:\n",
            "Hispanic/Latino           AUROC 95% CI: [0.7979, 0.8130]\n",
            "Non-Hispanic Asian        AUROC 95% CI: [0.9247, 0.9334]\n",
            "Non-Hispanic Black        AUROC 95% CI: [0.9442, 0.9542]\n",
            "Non-Hispanic White        AUROC 95% CI: [0.8944, 0.9020]\n",
            "\n",
            "MIMIC - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.9101, 0.9162]\n",
            "wAUPRC 95% CI: [0.8789, 0.8863]\n",
            "\n",
            "MIMIC - Per-Class AUROC 95% CIs:\n",
            "Hispanic/Latino           AUROC 95% CI: [0.6855, 0.7102]\n",
            "Non-Hispanic Asian        AUROC 95% CI: [0.9375, 0.9496]\n",
            "Non-Hispanic Black        AUROC 95% CI: [0.9438, 0.9493]\n",
            "Non-Hispanic White        AUROC 95% CI: [0.9163, 0.9231]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIMIC"
      ],
      "metadata": {
        "id": "ZH6Jst0bLwt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "def evaluate_race_csv(csv_file_path, label_name):\n",
        "    try:\n",
        "        predict_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Parse the stringified list into a real list\n",
        "        predict_df['Race/Ethnicity_Probability'] = predict_df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "\n",
        "        # Encode the textual labels into integers\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true = label_encoder.fit_transform(predict_df['Race/Ethnicity'])\n",
        "\n",
        "        # Get the list of probabilities\n",
        "        y_score = predict_df['Race/Ethnicity_Probability'].tolist()\n",
        "\n",
        "        # Compute weighted AUROC\n",
        "        wAUROC = metrics.roc_auc_score(\n",
        "            y_true,\n",
        "            y_score,\n",
        "            multi_class='ovr',\n",
        "            average='weighted'\n",
        "        )\n",
        "\n",
        "        # Compute weighted AUPRC\n",
        "        wAUPRC = metrics.average_precision_score(\n",
        "            y_true,\n",
        "            y_score,\n",
        "            average='weighted'\n",
        "        )\n",
        "\n",
        "        print(f'{label_name} - wAUROC: {wAUROC:.4f}, wAUPRC: {wAUPRC:.4f}')\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "\n",
        "# Define files and labels\n",
        "csv_files_with_labels = [\n",
        "    ('/content/mimic_race_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "# Run evaluations\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    evaluate_race_csv(file_path, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbeKWFjxNpFo",
        "outputId": "b40f8f63-c996-456a-a29a-bdc4ab54706f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MIMIC - wAUROC: 0.9046, wAUPRC: 0.8439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Asian', 'Black', 'Hispanic/Latino', 'Other', 'White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Asian',\n",
        "    'Non-Hispanic Asian': 'Asian',\n",
        "    'Black': 'Black',\n",
        "    'Non-Hispanic Black': 'Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'White',\n",
        "    'Non-Hispanic White': 'White'\n",
        "}\n",
        "\n",
        "def evaluate_race_csv(csv_file_path, label_name):\n",
        "    try:\n",
        "        predict_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Parse probabilities and standardize race labels\n",
        "        predict_df['Race/Ethnicity_Probability'] = predict_df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        predict_df['Race/Ethnicity'] = predict_df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        # Encode the textual labels into integers\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true = label_encoder.fit_transform(predict_df['Race/Ethnicity'])\n",
        "        classes = label_encoder.classes_\n",
        "\n",
        "        # Get the list of probabilities and convert to numpy array\n",
        "        y_score = np.array(predict_df['Race/Ethnicity_Probability'].tolist())\n",
        "\n",
        "        # Calculate class prevalences (weights)\n",
        "        class_counts = np.bincount(y_true)\n",
        "        class_weights = class_counts / len(y_true)\n",
        "\n",
        "        # Initialize lists to store per-class metrics\n",
        "        auroc_scores = []\n",
        "        auprc_scores = []\n",
        "\n",
        "        # Compute metrics for each class using one-vs-rest approach\n",
        "        for class_idx in range(len(classes)):\n",
        "          if class_idx == 3 and label_name == 'CheXpert':\n",
        "            # Create binary labels for current class\n",
        "            y_true_binary = (y_true == 3).astype(int)\n",
        "\n",
        "            # Get probabilities for current class\n",
        "            y_score_class = y_score[:, 4]\n",
        "\n",
        "            # Compute AUROC\n",
        "            if len(np.unique(y_true_binary)) >= 2:  # Need at least one positive and one negative sample\n",
        "                auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                auroc_scores.append(auroc)\n",
        "            else:\n",
        "                auroc_scores.append(np.nan)\n",
        "            auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "            auprc_scores.append(auprc)\n",
        "          else:\n",
        "            # Create binary labels for current class\n",
        "            y_true_binary = (y_true == class_idx).astype(int)\n",
        "\n",
        "            # Get probabilities for current class\n",
        "            y_score_class = y_score[:, class_idx]\n",
        "\n",
        "            # Compute AUROC\n",
        "            if len(np.unique(y_true_binary)) >= 2:  # Need at least one positive and one negative sample\n",
        "                auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                auroc_scores.append(auroc)\n",
        "            else:\n",
        "                auroc_scores.append(np.nan)\n",
        "\n",
        "            # Compute AUPRC\n",
        "            auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "            auprc_scores.append(auprc)\n",
        "\n",
        "        # Calculate manually weighted averages (ignoring NaN values)\n",
        "        valid_auroc = ~np.isnan(auroc_scores)\n",
        "        weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "\n",
        "        weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "        # Print results\n",
        "        print(f'\\n{label_name} Results:')\n",
        "        print('Class\\t\\tPrevalence\\tAUROC\\t\\tAUPRC')\n",
        "        for i, class_name in enumerate(classes):\n",
        "            print(f'{class_name:<15}{class_weights[i]:.4f}\\t\\t{auroc_scores[i]:.4f}\\t\\t{auprc_scores[i]:.4f}')\n",
        "\n",
        "        print(f'\\n{label_name} - Manually Weighted Average - wAUROC: {weighted_auroc:.4f}, wAUPRC: {weighted_auprc:.4f}')\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "\n",
        "# Define files and labels\n",
        "csv_files_with_labels = [\n",
        "    ('/content/mimic_race_chexpert_race_v2_test.csv', 'CheXpert'),\n",
        "    ('/content/mimic_race_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "# Run evaluations\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    evaluate_race_csv(file_path, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9joY0jBRzBB",
        "outputId": "f042b09f-eb60-4254-9fcd-b2a3daf99904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert Results:\n",
            "Class\t\tPrevalence\tAUROC\t\tAUPRC\n",
            "Asian          0.1432\t\t0.8565\t\t0.5727\n",
            "Black          0.0573\t\t0.9222\t\t0.5933\n",
            "Hispanic/Latino0.1360\t\t0.7098\t\t0.2902\n",
            "White          0.6635\t\t0.8349\t\t0.8933\n",
            "\n",
            "CheXpert - Manually Weighted Average - wAUROC: 0.8259, wAUPRC: 0.7481\n",
            "\n",
            "MIMIC Results:\n",
            "Class\t\tPrevalence\tAUROC\t\tAUPRC\n",
            "Asian          0.0372\t\t0.9246\t\t0.5131\n",
            "Black          0.1711\t\t0.9487\t\t0.8121\n",
            "Hispanic/Latino0.0533\t\t0.7892\t\t0.1988\n",
            "Other          0.0301\t\t0.6410\t\t0.0517\n",
            "White          0.7083\t\t0.9128\t\t0.9511\n",
            "\n",
            "MIMIC - Manually Weighted Average - wAUROC: 0.9046, wAUPRC: 0.8439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Asian', 'Black', 'Hispanic/Latino', 'Other', 'White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Asian',\n",
        "    'Non-Hispanic Asian': 'Asian',\n",
        "    'Black': 'Black',\n",
        "    'Non-Hispanic Black': 'Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'White',\n",
        "    'Non-Hispanic White': 'White'\n",
        "}\n",
        "\n",
        "def bootstrap_race_metrics(csv_file_path, label_name, n_iterations=1000, random_state=2025):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        df['Race/Ethnicity_Probability'] = df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        df['Race/Ethnicity'] = df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true_all = label_encoder.fit_transform(df['Race/Ethnicity'])\n",
        "        y_score_all = np.array(df['Race/Ethnicity_Probability'].tolist())\n",
        "        classes = label_encoder.classes_\n",
        "        n_classes = len(classes)\n",
        "\n",
        "        data = pd.DataFrame({'y_true': y_true_all.tolist(), 'y_score': y_score_all.tolist()})\n",
        "        rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "        auroc_bootstrap = []\n",
        "        auprc_bootstrap = []\n",
        "\n",
        "        for _ in range(n_iterations):\n",
        "            sample = data.sample(n=len(data), replace=True, random_state=rng.integers(1e9))\n",
        "            y_true = np.array(sample['y_true'].tolist())\n",
        "            y_score = np.array(sample['y_score'].tolist())\n",
        "\n",
        "            auroc_scores = []\n",
        "            auprc_scores = []\n",
        "            class_counts = np.bincount(y_true, minlength=n_classes)\n",
        "            class_weights = class_counts / len(y_true)\n",
        "\n",
        "            for class_idx in range(n_classes):\n",
        "                y_true_binary = (y_true == class_idx).astype(int)\n",
        "\n",
        "                # Special case for CheXpert \"Other\" class index 3\n",
        "                if class_idx == 3 and label_name == 'CheXpert':\n",
        "                    y_score_class = y_score[:, 4]\n",
        "                else:\n",
        "                    y_score_class = y_score[:, class_idx]\n",
        "\n",
        "                if len(np.unique(y_true_binary)) >= 2:\n",
        "                    auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                else:\n",
        "                    auroc = np.nan\n",
        "                auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "\n",
        "                auroc_scores.append(auroc)\n",
        "                auprc_scores.append(auprc)\n",
        "\n",
        "            valid_auroc = ~np.isnan(auroc_scores)\n",
        "            weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "            weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "            auroc_bootstrap.append(weighted_auroc)\n",
        "            auprc_bootstrap.append(weighted_auprc)\n",
        "\n",
        "        auroc_ci = np.percentile(auroc_bootstrap, [2.5, 97.5])\n",
        "        auprc_ci = np.percentile(auprc_bootstrap, [2.5, 97.5])\n",
        "\n",
        "        print(f\"\\n{label_name} - Bootstrapped Weighted Metrics:\")\n",
        "        print(f\"wAUROC 95% CI: [{auroc_ci[0]:.4f}, {auroc_ci[1]:.4f}]\")\n",
        "        print(f\"wAUPRC 95% CI: [{auprc_ci[0]:.4f}, {auprc_ci[1]:.4f}]\")\n",
        "\n",
        "        return auroc_ci, auprc_ci\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Run bootstrap evaluation on your new files\n",
        "csv_files_with_labels = [\n",
        "    ('/content/mimic_race_chexpert_race_v2_test.csv', 'CheXpert'),\n",
        "    ('/content/mimic_race_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "for path, label in csv_files_with_labels:\n",
        "    bootstrap_race_metrics(path, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWAVxkn2I12C",
        "outputId": "c6205bcf-de60-4253-e27b-d6a393ec22f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.8210, 0.8304]\n",
            "wAUPRC 95% CI: [0.7425, 0.7540]\n",
            "\n",
            "MIMIC - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.9015, 0.9078]\n",
            "wAUPRC 95% CI: [0.8401, 0.8480]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Asian', 'Black', 'Hispanic/Latino', 'Other', 'White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Asian',\n",
        "    'Non-Hispanic Asian': 'Asian',\n",
        "    'Black': 'Black',\n",
        "    'Non-Hispanic Black': 'Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'White',\n",
        "    'Non-Hispanic White': 'White'\n",
        "}\n",
        "\n",
        "def bootstrap_race_metrics(csv_file_path, label_name, n_iterations=1000, random_state=2025):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        df['Race/Ethnicity_Probability'] = df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        df['Race/Ethnicity'] = df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true_all = label_encoder.fit_transform(df['Race/Ethnicity'])\n",
        "        y_score_all = np.array(df['Race/Ethnicity_Probability'].tolist())\n",
        "        classes = label_encoder.classes_\n",
        "        n_classes = len(classes)\n",
        "\n",
        "        data = pd.DataFrame({'y_true': y_true_all.tolist(), 'y_score': y_score_all.tolist()})\n",
        "        rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "        # Store AUROC and AUPRC for weighted and each class\n",
        "        auroc_bootstrap = []\n",
        "        auprc_bootstrap = []\n",
        "        per_class_aurocs = [[] for _ in range(n_classes)]\n",
        "\n",
        "        for _ in range(n_iterations):\n",
        "            sample = data.sample(n=len(data), replace=True, random_state=rng.integers(1e9))\n",
        "            y_true = np.array(sample['y_true'].tolist())\n",
        "            y_score = np.array(sample['y_score'].tolist())\n",
        "\n",
        "            auroc_scores = []\n",
        "            auprc_scores = []\n",
        "            class_counts = np.bincount(y_true, minlength=n_classes)\n",
        "            class_weights = class_counts / len(y_true)\n",
        "\n",
        "            for class_idx in range(n_classes):\n",
        "                y_true_binary = (y_true == class_idx).astype(int)\n",
        "\n",
        "                # Special case for CheXpert \"Other\" class index 3\n",
        "                if class_idx == 3 and label_name == 'CheXpert':\n",
        "                    y_score_class = y_score[:, 4]\n",
        "                else:\n",
        "                    y_score_class = y_score[:, class_idx]\n",
        "\n",
        "                if len(np.unique(y_true_binary)) >= 2:\n",
        "                    auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                else:\n",
        "                    auroc = np.nan\n",
        "                auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "\n",
        "                auroc_scores.append(auroc)\n",
        "                auprc_scores.append(auprc)\n",
        "\n",
        "                # Collect for CI computation\n",
        "                per_class_aurocs[class_idx].append(auroc)\n",
        "\n",
        "            valid_auroc = ~np.isnan(auroc_scores)\n",
        "            weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "            weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "            auroc_bootstrap.append(weighted_auroc)\n",
        "            auprc_bootstrap.append(weighted_auprc)\n",
        "\n",
        "        # Compute weighted CIs\n",
        "        auroc_ci = np.percentile(auroc_bootstrap, [2.5, 97.5])\n",
        "        auprc_ci = np.percentile(auprc_bootstrap, [2.5, 97.5])\n",
        "\n",
        "        print(f\"\\n{label_name} - Bootstrapped Weighted Metrics:\")\n",
        "        print(f\"wAUROC 95% CI: [{auroc_ci[0]:.4f}, {auroc_ci[1]:.4f}]\")\n",
        "        print(f\"wAUPRC 95% CI: [{auprc_ci[0]:.4f}, {auprc_ci[1]:.4f}]\")\n",
        "\n",
        "        # Compute and print per-class AUROC CIs\n",
        "        print(f\"\\n{label_name} - Per-Class AUROC 95% Confidence Intervals:\")\n",
        "        for i, class_name in enumerate(classes):\n",
        "            class_aurocs = [x for x in per_class_aurocs[i] if not np.isnan(x)]\n",
        "            if len(class_aurocs) > 0:\n",
        "                ci_low, ci_high = np.percentile(class_aurocs, [2.5, 97.5])\n",
        "                print(f\"{class_name:<20}: [{ci_low:.4f}, {ci_high:.4f}]\")\n",
        "            else:\n",
        "                print(f\"{class_name:<20}: AUROC CI not computable (insufficient variation)\")\n",
        "\n",
        "        return auroc_ci, auprc_ci\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Run bootstrap evaluation\n",
        "csv_files_with_labels = [\n",
        "    ('/content/mimic_race_chexpert_race_v2_test.csv', 'CheXpert'),\n",
        "    ('/content/mimic_race_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "for path, label in csv_files_with_labels:\n",
        "    bootstrap_race_metrics(path, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hZu1-YaRP4p",
        "outputId": "1874acc2-9912-4527-8a1d-fa09a819a581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.8210, 0.8304]\n",
            "wAUPRC 95% CI: [0.7425, 0.7540]\n",
            "\n",
            "CheXpert - Per-Class AUROC 95% Confidence Intervals:\n",
            "Asian               : [0.8506, 0.8631]\n",
            "Black               : [0.9152, 0.9290]\n",
            "Hispanic/Latino     : [0.7014, 0.7179]\n",
            "White               : [0.8296, 0.8396]\n",
            "\n",
            "MIMIC - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.9015, 0.9078]\n",
            "wAUPRC 95% CI: [0.8401, 0.8480]\n",
            "\n",
            "MIMIC - Per-Class AUROC 95% Confidence Intervals:\n",
            "Asian               : [0.9187, 0.9311]\n",
            "Black               : [0.9462, 0.9512]\n",
            "Hispanic/Latino     : [0.7799, 0.7978]\n",
            "Other               : [0.6250, 0.6558]\n",
            "White               : [0.9095, 0.9160]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We also trained a classifier on CheXpert where only non-Hispanic Asian, non-Hispanic Black and non-Hispanic White individuals were included. These are the results"
      ],
      "metadata": {
        "id": "1qNsZAntYgwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CheXpert"
      ],
      "metadata": {
        "id": "m749_JEwY77U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Hispanic/Latino', 'Non-Hispanic Asian', 'Non-Hispanic Black',  'Non-Hispanic White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Non-Hispanic Asian',\n",
        "    'Non-Hispanic Asian': 'Non-Hispanic Asian',\n",
        "    'Black': 'Non-Hispanic Black',\n",
        "    'Non-Hispanic Black': 'Non-Hispanic Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'Non-Hispanic White',\n",
        "    'Non-Hispanic White': 'Non-Hispanic White'\n",
        "}\n",
        "\n",
        "def evaluate_race_csv(csv_file_path, label_name):\n",
        "    try:\n",
        "        predict_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Parse the stringified list into a real list\n",
        "        predict_df['Race/Ethnicity_Probability'] = predict_df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        predict_df['Race/Ethnicity'] = predict_df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        predict_df = predict_df[predict_df['Race/Ethnicity'] != 'Other']\n",
        "        predict_df = predict_df[predict_df['Race/Ethnicity'] != 'Hispanic/Latino']\n",
        "\n",
        "        # Encode the textual labels into integers\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true = label_encoder.fit_transform(predict_df['Race/Ethnicity'])\n",
        "\n",
        "        # Get the list of probabilities\n",
        "        y_score = predict_df['Race/Ethnicity_Probability'].tolist()\n",
        "\n",
        "        # Compute weighted AUROC\n",
        "        wAUROC = metrics.roc_auc_score(\n",
        "            y_true,\n",
        "            y_score,\n",
        "            multi_class='ovr',\n",
        "            average='weighted'\n",
        "        )\n",
        "\n",
        "        # Compute weighted AUPRC\n",
        "        wAUPRC = metrics.average_precision_score(\n",
        "            y_true,\n",
        "            y_score,\n",
        "            average='weighted'\n",
        "        )\n",
        "\n",
        "        print(f'{label_name} - wAUROC: {wAUROC:.4f}, wAUPRC: {wAUPRC:.4f}')\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "\n",
        "# Define files and labels\n",
        "csv_files_with_labels = [\n",
        "    ('/content/chexpert_race1_chexpert_test.csv', 'CheXpert'),\n",
        "    ('/content/chexpert_race1_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "# Run evaluations\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    evaluate_race_csv(file_path, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DGR9eueYqeg",
        "outputId": "64984c7c-8b69-4811-b73d-d6adba9860cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CheXpert - wAUROC: 0.9362, wAUPRC: 0.9312\n",
            "MIMIC - wAUROC: 0.9490, wAUPRC: 0.9454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Hispanic/Latino', 'Non-Hispanic Asian', 'Non-Hispanic Black',  'Non-Hispanic White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Non-Hispanic Asian',\n",
        "    'Non-Hispanic Asian': 'Non-Hispanic Asian',\n",
        "    'Black': 'Non-Hispanic Black',\n",
        "    'Non-Hispanic Black': 'Non-Hispanic Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'Non-Hispanic White',\n",
        "    'Non-Hispanic White': 'Non-Hispanic White'\n",
        "}\n",
        "\n",
        "def evaluate_race_csv(csv_file_path, label_name):\n",
        "    try:\n",
        "        predict_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Parse the stringified list into a real list\n",
        "        predict_df['Race/Ethnicity_Probability'] = predict_df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        predict_df['Race/Ethnicity'] = predict_df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        predict_df = predict_df[predict_df['Race/Ethnicity'] != 'Other']\n",
        "        predict_df = predict_df[predict_df['Race/Ethnicity'] != 'Hispanic/Latino']\n",
        "\n",
        "\n",
        "        # Encode the textual labels into integers\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true = label_encoder.fit_transform(predict_df['Race/Ethnicity'])\n",
        "        classes = label_encoder.classes_\n",
        "\n",
        "        # Get the list of probabilities and convert to numpy array\n",
        "        y_score = np.array(predict_df['Race/Ethnicity_Probability'].tolist())\n",
        "\n",
        "        # Calculate class prevalences (weights)\n",
        "        class_counts = np.bincount(y_true)\n",
        "        class_weights = class_counts / len(y_true)\n",
        "\n",
        "        # Initialize lists to store per-class metrics\n",
        "        auroc_scores = []\n",
        "        auprc_scores = []\n",
        "\n",
        "        # Compute metrics for each class using one-vs-rest approach\n",
        "        for class_idx in range(len(classes)):\n",
        "            # Create binary labels for current class\n",
        "            y_true_binary = (y_true == class_idx).astype(int)\n",
        "\n",
        "            # Get probabilities for current class\n",
        "            y_score_class = y_score[:, class_idx]\n",
        "\n",
        "            # Compute AUROC\n",
        "            if len(np.unique(y_true_binary)) >= 2:  # Need at least one positive and one negative sample\n",
        "                auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                auroc_scores.append(auroc)\n",
        "            else:\n",
        "                auroc_scores.append(np.nan)\n",
        "\n",
        "            # Compute AUPRC\n",
        "            auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "            auprc_scores.append(auprc)\n",
        "\n",
        "        # Calculate manually weighted averages (ignoring NaN values)\n",
        "        valid_auroc = ~np.isnan(auroc_scores)\n",
        "        weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "\n",
        "        weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "        # Print results\n",
        "        print(f'\\n{label_name} Results:')\n",
        "        print('Class\\t\\tPrevalence\\tAUROC\\t\\tAUPRC')\n",
        "        for i, class_name in enumerate(classes):\n",
        "            print(f'{class_name:<15}{class_weights[i]:.4f}\\t\\t{auroc_scores[i]:.4f}\\t\\t{auprc_scores[i]:.4f}')\n",
        "\n",
        "        print(f'\\n{label_name} - Manually Weighted Average - wAUROC: {weighted_auroc:.4f}, wAUPRC: {weighted_auprc:.4f}')\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "\n",
        "# Define files and labels\n",
        "csv_files_with_labels = [\n",
        "    ('/content/chexpert_race1_chexpert_test.csv', 'CheXpert'),\n",
        "    ('/content/chexpert_race1_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "# Run evaluations\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    evaluate_race_csv(file_path, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn-6qbp5ZMwh",
        "outputId": "7d298f55-f050-4405-da53-3d3b1c1fa0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert Results:\n",
            "Class\t\tPrevalence\tAUROC\t\tAUPRC\n",
            "Non-Hispanic Asian0.1702\t\t0.9435\t\t0.8318\n",
            "Non-Hispanic Black0.0722\t\t0.9293\t\t0.7120\n",
            "Non-Hispanic White0.7576\t\t0.9353\t\t0.9744\n",
            "\n",
            "CheXpert - Manually Weighted Average - wAUROC: 0.9362, wAUPRC: 0.9312\n",
            "\n",
            "MIMIC Results:\n",
            "Class\t\tPrevalence\tAUROC\t\tAUPRC\n",
            "Non-Hispanic Asian0.0406\t\t0.9325\t\t0.6014\n",
            "Non-Hispanic Black0.1867\t\t0.9533\t\t0.8680\n",
            "Non-Hispanic White0.7727\t\t0.9489\t\t0.9822\n",
            "\n",
            "MIMIC - Manually Weighted Average - wAUROC: 0.9490, wAUPRC: 0.9454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Non-Hispanic Asian', 'Non-Hispanic Black',  'Non-Hispanic White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Non-Hispanic Asian',\n",
        "    'Non-Hispanic Asian': 'Non-Hispanic Asian',\n",
        "    'Black': 'Non-Hispanic Black',\n",
        "    'Non-Hispanic Black': 'Non-Hispanic Black',\n",
        "    'Hispanic/Latino': 'Other',\n",
        "    'Other': 'Other',\n",
        "    'White': 'Non-Hispanic White',\n",
        "    'Non-Hispanic White': 'Non-Hispanic White'\n",
        "}\n",
        "\n",
        "def bootstrap_race_metrics(csv_file_path, label_name, n_iterations=1000, random_state=2025):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        df['Race/Ethnicity_Probability'] = df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        df['Race/Ethnicity'] = df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        df = df[df['Race/Ethnicity'] != 'Other'].copy()\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true_all = label_encoder.fit_transform(df['Race/Ethnicity'])\n",
        "        y_score_all = np.array(df['Race/Ethnicity_Probability'].tolist())\n",
        "        classes = label_encoder.classes_\n",
        "        n_classes = len(classes)\n",
        "\n",
        "        data = pd.DataFrame({'y_true': y_true_all.tolist(), 'y_score': y_score_all.tolist()})\n",
        "        rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "        auroc_bootstrap = []\n",
        "        auprc_bootstrap = []\n",
        "\n",
        "        for _ in range(n_iterations):\n",
        "            sample = data.sample(n=len(data), replace=True, random_state=rng.integers(1e9))\n",
        "            y_true = np.array(sample['y_true'].tolist())\n",
        "            y_score = np.array(sample['y_score'].tolist())\n",
        "\n",
        "            auroc_scores = []\n",
        "            auprc_scores = []\n",
        "            class_counts = np.bincount(y_true, minlength=n_classes)\n",
        "            class_weights = class_counts / len(y_true)\n",
        "\n",
        "            for class_idx in range(n_classes):\n",
        "                y_true_binary = (y_true == class_idx).astype(int)\n",
        "                y_score_class = y_score[:, class_idx]\n",
        "\n",
        "                if len(np.unique(y_true_binary)) >= 2:\n",
        "                    auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                else:\n",
        "                    auroc = np.nan\n",
        "                auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "\n",
        "                auroc_scores.append(auroc)\n",
        "                auprc_scores.append(auprc)\n",
        "\n",
        "            # Weighted average ignoring NaN in AUROC\n",
        "            valid_auroc = ~np.isnan(auroc_scores)\n",
        "            weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "            weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "            auroc_bootstrap.append(weighted_auroc)\n",
        "            auprc_bootstrap.append(weighted_auprc)\n",
        "\n",
        "        # Compute confidence intervals\n",
        "        auroc_ci = np.percentile(auroc_bootstrap, [2.5, 97.5])\n",
        "        auprc_ci = np.percentile(auprc_bootstrap, [2.5, 97.5])\n",
        "\n",
        "        print(f\"\\n{label_name} - Bootstrapped Weighted Metrics:\")\n",
        "        print(f\"wAUROC 95% CI: [{auroc_ci[0]:.4f}, {auroc_ci[1]:.4f}]\")\n",
        "        print(f\"wAUPRC 95% CI: [{auprc_ci[0]:.4f}, {auprc_ci[1]:.4f}]\")\n",
        "\n",
        "        return auroc_ci, auprc_ci\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Run bootstrap evaluation\n",
        "csv_files_with_labels = [\n",
        "    ('/content/chexpert_race1_chexpert_test.csv', 'CheXpert'),\n",
        "    ('/content/chexpert_race1_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    bootstrap_race_metrics(file_path, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF5ZKci2KjNp",
        "outputId": "899448c2-406d-4b88-916f-76452c604f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.9327, 0.9396]\n",
            "wAUPRC 95% CI: [0.9280, 0.9345]\n",
            "\n",
            "MIMIC - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.9466, 0.9513]\n",
            "wAUPRC 95% CI: [0.9433, 0.9477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Non-Hispanic Asian', 'Non-Hispanic Black',  'Non-Hispanic White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Non-Hispanic Asian',\n",
        "    'Non-Hispanic Asian': 'Non-Hispanic Asian',\n",
        "    'Black': 'Non-Hispanic Black',\n",
        "    'Non-Hispanic Black': 'Non-Hispanic Black',\n",
        "    'Hispanic/Latino': 'Other',\n",
        "    'Other': 'Other',\n",
        "    'White': 'Non-Hispanic White',\n",
        "    'Non-Hispanic White': 'Non-Hispanic White'\n",
        "}\n",
        "\n",
        "def bootstrap_race_metrics(csv_file_path, label_name, n_iterations=1000, random_state=2025):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        df['Race/Ethnicity_Probability'] = df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        df['Race/Ethnicity'] = df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        df = df[df['Race/Ethnicity'] != 'Other'].copy()\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true_all = label_encoder.fit_transform(df['Race/Ethnicity'])\n",
        "        y_score_all = np.array(df['Race/Ethnicity_Probability'].tolist())\n",
        "        classes = label_encoder.classes_\n",
        "        n_classes = len(classes)\n",
        "\n",
        "        data = pd.DataFrame({'y_true': y_true_all.tolist(), 'y_score': y_score_all.tolist()})\n",
        "        rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "        auroc_bootstrap = []\n",
        "        auprc_bootstrap = []\n",
        "        per_class_aurocs = [[] for _ in range(n_classes)]\n",
        "\n",
        "        for _ in range(n_iterations):\n",
        "            sample = data.sample(n=len(data), replace=True, random_state=rng.integers(1e9))\n",
        "            y_true = np.array(sample['y_true'].tolist())\n",
        "            y_score = np.array(sample['y_score'].tolist())\n",
        "\n",
        "            auroc_scores = []\n",
        "            auprc_scores = []\n",
        "            class_counts = np.bincount(y_true, minlength=n_classes)\n",
        "            class_weights = class_counts / len(y_true)\n",
        "\n",
        "            for class_idx in range(n_classes):\n",
        "                y_true_binary = (y_true == class_idx).astype(int)\n",
        "                y_score_class = y_score[:, class_idx]\n",
        "\n",
        "                if len(np.unique(y_true_binary)) >= 2:\n",
        "                    auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                else:\n",
        "                    auroc = np.nan\n",
        "                auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "\n",
        "                auroc_scores.append(auroc)\n",
        "                auprc_scores.append(auprc)\n",
        "\n",
        "                # Store for per-class CI\n",
        "                per_class_aurocs[class_idx].append(auroc)\n",
        "\n",
        "            valid_auroc = ~np.isnan(auroc_scores)\n",
        "            weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "            weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "            auroc_bootstrap.append(weighted_auroc)\n",
        "            auprc_bootstrap.append(weighted_auprc)\n",
        "\n",
        "        auroc_ci = np.percentile(auroc_bootstrap, [2.5, 97.5])\n",
        "        auprc_ci = np.percentile(auprc_bootstrap, [2.5, 97.5])\n",
        "\n",
        "        print(f\"\\n{label_name} - Bootstrapped Weighted Metrics:\")\n",
        "        print(f\"wAUROC 95% CI: [{auroc_ci[0]:.4f}, {auroc_ci[1]:.4f}]\")\n",
        "        print(f\"wAUPRC 95% CI: [{auprc_ci[0]:.4f}, {auprc_ci[1]:.4f}]\\n\")\n",
        "\n",
        "        # Per-class AUROC confidence intervals\n",
        "        print(f\"{label_name} - Per-Class AUROC 95% CIs:\")\n",
        "        for i, class_name in enumerate(classes):\n",
        "            class_aurocs = np.array(per_class_aurocs[i])\n",
        "            class_aurocs = class_aurocs[~np.isnan(class_aurocs)]\n",
        "            if len(class_aurocs) == 0:\n",
        "                print(f\"{class_name:<25} Insufficient data for AUROC\")\n",
        "            else:\n",
        "                ci = np.percentile(class_aurocs, [2.5, 97.5])\n",
        "                print(f\"{class_name:<25} AUROC 95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
        "\n",
        "        return auroc_ci, auprc_ci\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Run bootstrap evaluation\n",
        "csv_files_with_labels = [\n",
        "    ('/content/chexpert_race1_chexpert_test.csv', 'CheXpert'),\n",
        "    ('/content/chexpert_race1_mimic_test.csv', 'MIMIC')\n",
        "]\n",
        "\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    bootstrap_race_metrics(file_path, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDzOOdTOR0jm",
        "outputId": "5f6b9a5a-7e18-46b2-a86d-e12b35387115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.9327, 0.9396]\n",
            "wAUPRC 95% CI: [0.9280, 0.9345]\n",
            "\n",
            "CheXpert - Per-Class AUROC 95% CIs:\n",
            "Non-Hispanic Asian        AUROC 95% CI: [0.9397, 0.9470]\n",
            "Non-Hispanic Black        AUROC 95% CI: [0.9223, 0.9360]\n",
            "Non-Hispanic White        AUROC 95% CI: [0.9317, 0.9387]\n",
            "\n",
            "MIMIC - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.9466, 0.9513]\n",
            "wAUPRC 95% CI: [0.9433, 0.9477]\n",
            "\n",
            "MIMIC - Per-Class AUROC 95% CIs:\n",
            "Non-Hispanic Asian        AUROC 95% CI: [0.9256, 0.9389]\n",
            "Non-Hispanic Black        AUROC 95% CI: [0.9508, 0.9559]\n",
            "Non-Hispanic White        AUROC 95% CI: [0.9464, 0.9514]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIMIC"
      ],
      "metadata": {
        "id": "dAK73ftGZbfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Asian', 'Black', 'Hispanic/Latino', 'Other', 'White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Asian',\n",
        "    'Non-Hispanic Asian': 'Asian',\n",
        "    'Black': 'Black',\n",
        "    'Non-Hispanic Black': 'Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'White',\n",
        "    'Non-Hispanic White': 'White'\n",
        "}\n",
        "\n",
        "def evaluate_race_csv(csv_file_path, label_name):\n",
        "    try:\n",
        "        predict_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Parse probabilities and standardize race labels\n",
        "        predict_df['Race/Ethnicity_Probability'] = predict_df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        predict_df['Race/Ethnicity'] = predict_df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        # Encode the textual labels into integers\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true = label_encoder.fit_transform(predict_df['Race/Ethnicity'])\n",
        "        classes = label_encoder.classes_\n",
        "\n",
        "        # Get the list of probabilities and convert to numpy array\n",
        "        y_score = np.array(predict_df['Race/Ethnicity_Probability'].tolist())\n",
        "\n",
        "        # Calculate class prevalences (weights)\n",
        "        class_counts = np.bincount(y_true)\n",
        "        class_weights = class_counts / len(y_true)\n",
        "\n",
        "        # Initialize lists to store per-class metrics\n",
        "        auroc_scores = []\n",
        "        auprc_scores = []\n",
        "\n",
        "        # Compute metrics for each class using one-vs-rest approach\n",
        "        for class_idx in range(len(classes)):\n",
        "          if class_idx == 2 and label_name == 'CheXpert':\n",
        "            # Create binary labels for current class\n",
        "            y_true_binary = (y_true == 2).astype(int)\n",
        "\n",
        "            # Get probabilities for current class\n",
        "            y_score_class = y_score[:, 4]\n",
        "\n",
        "            # Compute AUROC\n",
        "            if len(np.unique(y_true_binary)) >= 2:  # Need at least one positive and one negative sample\n",
        "                auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                auroc_scores.append(auroc)\n",
        "            else:\n",
        "                auroc_scores.append(np.nan)\n",
        "            auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "            auprc_scores.append(auprc)\n",
        "          else:\n",
        "            # Create binary labels for current class\n",
        "            y_true_binary = (y_true == class_idx).astype(int)\n",
        "\n",
        "            # Get probabilities for current class\n",
        "            y_score_class = y_score[:, class_idx]\n",
        "\n",
        "            # Compute AUROC\n",
        "            if len(np.unique(y_true_binary)) >= 2:  # Need at least one positive and one negative sample\n",
        "                auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                auroc_scores.append(auroc)\n",
        "            else:\n",
        "                auroc_scores.append(np.nan)\n",
        "\n",
        "            # Compute AUPRC\n",
        "            auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "            auprc_scores.append(auprc)\n",
        "\n",
        "        # Calculate manually weighted averages (ignoring NaN values)\n",
        "        valid_auroc = ~np.isnan(auroc_scores)\n",
        "        weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "\n",
        "        weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "        # Print results\n",
        "        print(f'\\n{label_name} Results:')\n",
        "        print('Class\\t\\tPrevalence\\tAUROC\\t\\tAUPRC')\n",
        "        for i, class_name in enumerate(classes):\n",
        "            print(f'{class_name:<15}{class_weights[i]:.4f}\\t\\t{auroc_scores[i]:.4f}\\t\\t{auprc_scores[i]:.4f}')\n",
        "\n",
        "        print(f'\\n{label_name} - Manually Weighted Average - wAUROC: {weighted_auroc:.4f}, wAUPRC: {weighted_auprc:.4f}')\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "\n",
        "# Define files and labels\n",
        "csv_files_with_labels = [\n",
        "    ('/content/mimic_race_chexpert_race_v1_test.csv', 'CheXpert')\n",
        "]\n",
        "\n",
        "# Run evaluations\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    evaluate_race_csv(file_path, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBsetbEJZcZE",
        "outputId": "ae1dd092-90ad-4154-a757-b59951be0d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert Results:\n",
            "Class\t\tPrevalence\tAUROC\t\tAUPRC\n",
            "Asian          0.1702\t\t0.8773\t\t0.6741\n",
            "Black          0.0722\t\t0.9119\t\t0.6481\n",
            "White          0.7576\t\t0.8740\t\t0.9481\n",
            "\n",
            "CheXpert - Manually Weighted Average - wAUROC: 0.8773, wAUPRC: 0.8798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Asian', 'Black', 'Hispanic/Latino', 'Other', 'White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Asian',\n",
        "    'Non-Hispanic Asian': 'Asian',\n",
        "    'Black': 'Black',\n",
        "    'Non-Hispanic Black': 'Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'White',\n",
        "    'Non-Hispanic White': 'White'\n",
        "}\n",
        "\n",
        "def bootstrap_race_metrics(csv_file_path, label_name, n_iterations=1000, random_state=2025):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        df['Race/Ethnicity_Probability'] = df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        df['Race/Ethnicity'] = df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true_all = label_encoder.fit_transform(df['Race/Ethnicity'])\n",
        "        y_score_all = np.array(df['Race/Ethnicity_Probability'].tolist())\n",
        "        classes = label_encoder.classes_\n",
        "        n_classes = len(classes)\n",
        "\n",
        "        data = pd.DataFrame({'y_true': y_true_all.tolist(), 'y_score': y_score_all.tolist()})\n",
        "        rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "        auroc_bootstrap = []\n",
        "        auprc_bootstrap = []\n",
        "\n",
        "        for _ in range(n_iterations):\n",
        "            sample = data.sample(n=len(data), replace=True, random_state=rng.integers(1e9))\n",
        "            y_true = np.array(sample['y_true'].tolist())\n",
        "            y_score = np.array(sample['y_score'].tolist())\n",
        "\n",
        "            auroc_scores = []\n",
        "            auprc_scores = []\n",
        "            class_counts = np.bincount(y_true, minlength=n_classes)\n",
        "            class_weights = class_counts / len(y_true)\n",
        "\n",
        "            for class_idx in range(n_classes):\n",
        "                y_true_binary = (y_true == class_idx).astype(int)\n",
        "\n",
        "                # Special case for CheXpert \"Other\" class index 3\n",
        "                if class_idx == 2 and label_name == 'CheXpert':\n",
        "                    y_score_class = y_score[:, 4]\n",
        "                else:\n",
        "                    y_score_class = y_score[:, class_idx]\n",
        "\n",
        "                if len(np.unique(y_true_binary)) >= 2:\n",
        "                    auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                else:\n",
        "                    auroc = np.nan\n",
        "                auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "\n",
        "                auroc_scores.append(auroc)\n",
        "                auprc_scores.append(auprc)\n",
        "\n",
        "            valid_auroc = ~np.isnan(auroc_scores)\n",
        "            weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "            weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "            auroc_bootstrap.append(weighted_auroc)\n",
        "            auprc_bootstrap.append(weighted_auprc)\n",
        "\n",
        "        auroc_ci = np.percentile(auroc_bootstrap, [2.5, 97.5])\n",
        "        auprc_ci = np.percentile(auprc_bootstrap, [2.5, 97.5])\n",
        "\n",
        "        print(f\"\\n{label_name} - Bootstrapped Weighted Metrics:\")\n",
        "        print(f\"wAUROC 95% CI: [{auroc_ci[0]:.4f}, {auroc_ci[1]:.4f}]\")\n",
        "        print(f\"wAUPRC 95% CI: [{auprc_ci[0]:.4f}, {auprc_ci[1]:.4f}]\")\n",
        "\n",
        "        return auroc_ci, auprc_ci\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Run bootstrap evaluation\n",
        "csv_files_with_labels = [\n",
        "    ('/content/mimic_race_chexpert_race_v1_test.csv', 'CheXpert')\n",
        "]\n",
        "\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    bootstrap_race_metrics(file_path, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmfrQ8FpKr2h",
        "outputId": "6b269ddd-bd67-4439-820a-4ca4eee7a5df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.8724, 0.8823]\n",
            "wAUPRC 95% CI: [0.8755, 0.8846]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Standardized race/ethnicity categories\n",
        "STANDARD_RACES = ['Asian', 'Black', 'Hispanic/Latino', 'Other', 'White']\n",
        "\n",
        "# Mapping from original labels to standardized labels\n",
        "RACE_STANDARDIZATION = {\n",
        "    'Asian': 'Asian',\n",
        "    'Non-Hispanic Asian': 'Asian',\n",
        "    'Black': 'Black',\n",
        "    'Non-Hispanic Black': 'Black',\n",
        "    'Hispanic/Latino': 'Hispanic/Latino',\n",
        "    'Other': 'Other',\n",
        "    'White': 'White',\n",
        "    'Non-Hispanic White': 'White'\n",
        "}\n",
        "\n",
        "def bootstrap_race_metrics(csv_file_path, label_name, n_iterations=1000, random_state=2025):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        df['Race/Ethnicity_Probability'] = df['Race/Ethnicity_Probability'].apply(ast.literal_eval)\n",
        "        df['Race/Ethnicity'] = df['Race/Ethnicity'].map(RACE_STANDARDIZATION)\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_true_all = label_encoder.fit_transform(df['Race/Ethnicity'])\n",
        "        y_score_all = np.array(df['Race/Ethnicity_Probability'].tolist())\n",
        "        classes = label_encoder.classes_\n",
        "        n_classes = len(classes)\n",
        "\n",
        "        data = pd.DataFrame({'y_true': y_true_all.tolist(), 'y_score': y_score_all.tolist()})\n",
        "        rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "        # Store AUROC and AUPRC for weighted and each class\n",
        "        auroc_bootstrap = []\n",
        "        auprc_bootstrap = []\n",
        "        per_class_aurocs = [[] for _ in range(n_classes)]\n",
        "\n",
        "        for _ in range(n_iterations):\n",
        "            sample = data.sample(n=len(data), replace=True, random_state=rng.integers(1e9))\n",
        "            y_true = np.array(sample['y_true'].tolist())\n",
        "            y_score = np.array(sample['y_score'].tolist())\n",
        "\n",
        "            auroc_scores = []\n",
        "            auprc_scores = []\n",
        "            class_counts = np.bincount(y_true, minlength=n_classes)\n",
        "            class_weights = class_counts / len(y_true)\n",
        "\n",
        "            for class_idx in range(n_classes):\n",
        "                y_true_binary = (y_true == class_idx).astype(int)\n",
        "\n",
        "                # Special case for CheXpert \"Other\" class index 3\n",
        "                if class_idx == 2 and label_name == 'CheXpert':\n",
        "                    y_score_class = y_score[:, 4]\n",
        "                else:\n",
        "                    y_score_class = y_score[:, class_idx]\n",
        "\n",
        "                if len(np.unique(y_true_binary)) >= 2:\n",
        "                    auroc = metrics.roc_auc_score(y_true_binary, y_score_class)\n",
        "                else:\n",
        "                    auroc = np.nan\n",
        "                auprc = metrics.average_precision_score(y_true_binary, y_score_class)\n",
        "\n",
        "                auroc_scores.append(auroc)\n",
        "                auprc_scores.append(auprc)\n",
        "\n",
        "                # Collect for CI computation\n",
        "                per_class_aurocs[class_idx].append(auroc)\n",
        "\n",
        "            valid_auroc = ~np.isnan(auroc_scores)\n",
        "            weighted_auroc = np.sum(np.array(auroc_scores)[valid_auroc] * class_weights[valid_auroc]) / np.sum(class_weights[valid_auroc])\n",
        "            weighted_auprc = np.sum(np.array(auprc_scores) * class_weights) / np.sum(class_weights)\n",
        "\n",
        "            auroc_bootstrap.append(weighted_auroc)\n",
        "            auprc_bootstrap.append(weighted_auprc)\n",
        "\n",
        "        # Compute weighted CIs\n",
        "        auroc_ci = np.percentile(auroc_bootstrap, [2.5, 97.5])\n",
        "        auprc_ci = np.percentile(auprc_bootstrap, [2.5, 97.5])\n",
        "\n",
        "        print(f\"\\n{label_name} - Bootstrapped Weighted Metrics:\")\n",
        "        print(f\"wAUROC 95% CI: [{auroc_ci[0]:.4f}, {auroc_ci[1]:.4f}]\")\n",
        "        print(f\"wAUPRC 95% CI: [{auprc_ci[0]:.4f}, {auprc_ci[1]:.4f}]\")\n",
        "\n",
        "        # Compute and print per-class AUROC CIs\n",
        "        print(f\"\\n{label_name} - Per-Class AUROC 95% Confidence Intervals:\")\n",
        "        for i, class_name in enumerate(classes):\n",
        "            class_aurocs = [x for x in per_class_aurocs[i] if not np.isnan(x)]\n",
        "            if len(class_aurocs) > 0:\n",
        "                ci_low, ci_high = np.percentile(class_aurocs, [2.5, 97.5])\n",
        "                print(f\"{class_name:<20}: [{ci_low:.4f}, {ci_high:.4f}]\")\n",
        "            else:\n",
        "                print(f\"{class_name:<20}: AUROC CI not computable (insufficient variation)\")\n",
        "\n",
        "        return auroc_ci, auprc_ci\n",
        "\n",
        "    except (FileNotFoundError, KeyError, ValueError) as e:\n",
        "        print(f\"Error processing {csv_file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Run bootstrap evaluation\n",
        "csv_files_with_labels = [\n",
        "    ('/content/mimic_race_chexpert_race_v1_test.csv', 'CheXpert')\n",
        "]\n",
        "\n",
        "for file_path, label in csv_files_with_labels:\n",
        "    bootstrap_race_metrics(file_path, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N92U67lrSE6z",
        "outputId": "aa9353c5-31dc-4f60-dff7-e67c9b30edae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CheXpert - Bootstrapped Weighted Metrics:\n",
            "wAUROC 95% CI: [0.8724, 0.8823]\n",
            "wAUPRC 95% CI: [0.8755, 0.8846]\n",
            "\n",
            "CheXpert - Per-Class AUROC 95% Confidence Intervals:\n",
            "Asian               : [0.8718, 0.8832]\n",
            "Black               : [0.9042, 0.9191]\n",
            "White               : [0.8687, 0.8795]\n"
          ]
        }
      ]
    }
  ]
}